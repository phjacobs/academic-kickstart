{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSDMS_DeepLearning_Buscombe.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phjacobs/academic-kickstart/blob/master/CSDMS_DeepLearning_Buscombe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8x_6EThHiAr",
        "colab_type": "text"
      },
      "source": [
        "# Clinic: Landcover and landform classification using deep neural networks\n",
        "\n",
        "\n",
        "CSDMS\t2019\tAnnual\tMeeting:\tCSDMS\t3.0\t– Bridging\tBoundaries\n",
        "\n",
        "May\t21-23,\t2019\n",
        "\n",
        "### Daniel Buscombe\n",
        "\n",
        "Assistant Research Professor\n",
        "\n",
        "School of Earth and Sustainability\n",
        "\n",
        "School of Informatics, Computing and Cyber Systems\n",
        "\n",
        "\n",
        "Northern Arizona University, Flagstaff, AZ\n",
        "\n",
        "[Email](mailto:daniel.buscombe@nau.edu)\n",
        "[Web](www.danielbuscombe.com)\n",
        "[Google Scholar](https://scholar.google.com/citations?user=bwVl0NwAAAAJ&hl=en)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xnmwijxXGJd",
        "colab_type": "text"
      },
      "source": [
        "### What do I do with this notebook file?\n",
        "\n",
        "You probably have opened this from a direct link I sent you. If, however, you have been given this as a .ipynb file, you should save it to your personal google drive account. Once there, you should open it using [colaboratory](https://research.google.com/colaboratory/faq.html). \n",
        "\n",
        "When open in colaboratory, go to **Runtime** on the toolbar, then **Change runtime type**, and finally under **Hardware accelerator*** select **GPU** and hit **Save**. \n",
        "\n",
        "This will allow you to train models using a GPU, which is a necessity if you want model training to complete in a reasonable time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9kGyDe444s4",
        "colab_type": "text"
      },
      "source": [
        "## Purpose and content of this workshop\n",
        "\n",
        "\n",
        "### Purpose\n",
        "* Deep learning models are the current state-of-the-art for image recognition, segmentation, and classification tasks\n",
        "\n",
        "* The purpose of this clinic is to demonstrate a workflow for training pixelwise segmentation models using pairs of images and label images\n",
        "\n",
        "* The big limitation we have today is time, so we'll be using a small (manageable, and hopefully relevant) dataset and going over a workflow that you can explore in more detail on your own\n",
        "\n",
        "\n",
        "* I'll provide an overview of some tools and concepts; hopefully enough to equip you with the understanding you'll need to apply this to your own data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz_8NSPd5S3M",
        "colab_type": "code",
        "outputId": "1c3ab26d-ef4d-431e-ec2b-7b1a97ccef0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 50%; color: red;'><b>Warning: there is only so much we can achieve in 2 hours!</b></marquee>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<marquee style='width: 50%; color: red;'><b>Warning: there is only so much we can achieve in 2 hours!</b></marquee>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PGfFfhg6coC",
        "colab_type": "text"
      },
      "source": [
        "## Using jupyter notebooks\n",
        "\n",
        "[Jupyter](https://jupyter.org/) notebooks are a way to share executable code that can be run through a web browser. \n",
        "\n",
        "A notebook kernel is a computational engine that executes the code contained in a notebook. The ipython kernel executes python code. Kernels for many other languages also exist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_6CEb6krWQq",
        "colab_type": "text"
      },
      "source": [
        "You can navigate and list file contents in python using the ```os``` module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QRFouJxrWyv",
        "colab_type": "code",
        "outputId": "b9da58f5-fd27-4e07-aafa-068495ae2b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Egv1oKsO6Z",
        "colab_type": "text"
      },
      "source": [
        "You can also access shell commands using the bang (`!`) operator, which is usually simpler (typically using less code)\n",
        "\n",
        "\n",
        "For example to print working directory (`pwd`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrdWLPpGsPB5",
        "colab_type": "code",
        "outputId": "8c3d798b-0240-4b4f-8a84-d1932b97e6dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbQrdxUwrbxz",
        "colab_type": "text"
      },
      "source": [
        "We'll attempt to use minimal code in this workshop, which will necessitate the use of both python and bash (shell) commands\n",
        "\n",
        "\n",
        "There are a couple of different ways of running a python script and passing it variables. To demonstrate this, let's first make a simple script that accepts 2 variables and prints them to screen "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo3bjz6aOGMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var1 = 50\n",
        "var2 = 'jupyter_is_cool'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKCuGi3Rruzq",
        "colab_type": "text"
      },
      "source": [
        "```writefile``` is a so-called [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru2ewAN4OLmZ",
        "colab_type": "code",
        "outputId": "8034a4ee-7883-4e1a-b788-f4b78d718ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile test_script.py\n",
        "import sys\n",
        "script, var1, var2 = sys.argv\n",
        "print(\"Script name:\", script)\n",
        "print(\"First variable:\", var1)\n",
        "print(\"Second variable:\", var2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_script.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iudQYIEcr3Ct",
        "colab_type": "text"
      },
      "source": [
        "Below I use the ```bash``` command ```cat``` to show the contents of the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hIgiWGARBZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cat test_script.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIf9gi_iPkWh",
        "colab_type": "text"
      },
      "source": [
        "You can run the script using the ```run``` jupyter magic command. Variables are passed to the script using a $ symbol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrDzuTEMPkdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%run ./test_script.py $var1 $var2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bev55ipPRMYJ",
        "colab_type": "text"
      },
      "source": [
        "... or using bash. This is the convention we'll adopt in this clinic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBPQRcr4RMf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python3 test_script.py $var1 $var2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlimxOLvGSIp",
        "colab_type": "text"
      },
      "source": [
        "### What is Machine Learning?\n",
        "\n",
        "Machine Learning is a set of methods that allow computers to learn from data to make and improve predictions (for example cancer, weekly sales, credit default). \n",
        "\n",
        "Machine learning is a paradigm shift from “normal programming” where all instructions must be explicitly given to the computer to “indirect programming” that takes place through providing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx_jXRWMizj4",
        "colab_type": "text"
      },
      "source": [
        "![](https://christophm.github.io/interpretable-ml-book/images/programing-ml.png)\n",
        "[source](https://christophm.github.io/interpretable-ml-book/terminology.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs4aRZTiG61w",
        "colab_type": "text"
      },
      "source": [
        "![](https://christophm.github.io/interpretable-ml-book/images/learner.png)\n",
        "[source](https://christophm.github.io/interpretable-ml-book/terminology.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S31B7K0jjW--",
        "colab_type": "text"
      },
      "source": [
        "## Distinction between Machine and Deep Learning\n",
        "\n",
        "#### Machine learning \n",
        " * requires extracting features from data to input to the model\n",
        " * requires fine-tuning of model architecture\n",
        " * requires fine-tuning of model hyperparameters\n",
        " * performance tends to plateau with more data\n",
        " * lots of different models\n",
        "\n",
        "#### Deep learning ...\n",
        " * automatically extract features from data\n",
        " * automatically fine-tunes hyperparameters\n",
        " * performance doesn't tend to plateau with more data\n",
        " * requires fine-tuning of model architecture\n",
        " * just one model - the artificial neural network\n",
        "    \n",
        "   ![](https://images.xenonstack.com/blog/machine-learning-vs-deep-learning.png)\n",
        "   \n",
        "   [source](https://www.mathworks.com/videos/introduction-to-deep-learning-what-is-deep-learning--1489502328819.html)\n",
        "   \n",
        "\n",
        "Therefore, lots of interest from geoscientists\n",
        "\n",
        "* we don't have time or inclination to research models and fine-tune their parameters\n",
        "* we don't have time or inclination to develop optimal feature extraction techniques\n",
        "\n",
        "\n",
        "### Why use DL for image classification?\n",
        " \n",
        "* No ‘feature engineering’\n",
        "* Instead, hierarchy of features automatically learned from data\n",
        "* Potentially more powerful -  learns more abstract information\n",
        "\n",
        "\n",
        "### ML or AI?\n",
        "\n",
        "Difference between machine learning and AI:\n",
        "\n",
        "\"If it is written in Python, it's probably machine learning. If it is written in PowerPoint, it's probably AI\"\n",
        "\n",
        "[source](https://twitter.com/matvelloso/status/1065778379612282885)\n",
        "\n",
        "A less facetious definiton:\n",
        "\n",
        "\"“**Machine learning** is the study of computer algorithms that allow computer programs to automatically improve through experience.”\"\n",
        "\n",
        "“**Artificial intelligence** is the science and engineering of making computers behave in ways that, until recently, we thought required human intelligence.”\n",
        "\n",
        "The term AI is a moving target \"... based on those capabilities that humans possess but which machines do not.” \n",
        "\n",
        "[source](https://medium.com/datadriveninvestor/differences-between-ai-and-machine-learning-and-why-it-matters-1255b182fc6)\n",
        "\n",
        "\n",
        "### Problems DL is supposed overcome\n",
        "\n",
        "![](https://github.com/dbuscombe-usgs/cdi_dl_workshop/raw/67e8c84d0e0b89b024814ef2e8f3bed091ee0c4e/Day2/figs/Picture2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce5efc9Zj0hg",
        "colab_type": "text"
      },
      "source": [
        "Ok, we'll pick up lots of these threads, fill in details and explain the most important concepts as we go.\n",
        "\n",
        "\n",
        "So let's get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO6l5tl6GgjR",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the scripts and data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpKxMdPnUi5o",
        "colab_type": "text"
      },
      "source": [
        "The scripts and data are in a github repository called [```Semantic-Segmentation-Suite```](https://github.com/dbuscombe-usgs/Semantic-Segmentation-Suite), which is a fork of the [original repository](https://github.com/GeorgeSeif/Semantic-Segmentation-Suite) with a few minor alterations made by myself, and different data sets\n",
        "\n",
        "\n",
        "### Data\n",
        "\n",
        "* The example datasets are 30-cm aerial imagery of riverine and riparian environments of the Colorado River in Grand Canyon National Park an their associated label images. \n",
        "\n",
        "* The imagery is described in [Durning et al., 2017](https://pubs.usgs.gov/ds/1027/ds1027_introduction.html)\n",
        "\n",
        "* The pixel labels have been compiled by me, and are a somewhat coarse-resolution classification\n",
        "\n",
        "* There are two data sets, **GeomorphA** and **GeomorphB**\n",
        "\n",
        "* Due to time and compute limitations, the training data set is fairly small. A larger training data set is being compiled and will be available at a later date.\n",
        "\n",
        "\n",
        "#### GeomorphA\n",
        "The scene is classified into the following broad landcover/geomorphic categories: \n",
        "\n",
        "1. water\n",
        "\n",
        "> all types (aerated, rough, smooth)\n",
        "\n",
        "2. sand\n",
        "\n",
        "> all types (wet and dry open and sparsely vegetated sand, aeolian, beach and channel margin)\n",
        "\n",
        "3. talus and bedrock\n",
        "\n",
        "> all bedrock and coarse material deposited under gravity\n",
        "\n",
        "4. debris fan\n",
        "\n",
        "> all coarse clast- and matrix-supported sediment deposited by water\n",
        "\n",
        "5. vegetation\n",
        "\n",
        "> medium and dense stands, low and canopy\n",
        "\n",
        "6. other\n",
        "\n",
        "> boats and beach umbrellas!\n",
        "\n",
        "\n",
        "#### GeomorphB\n",
        "\n",
        "\n",
        "1. water\n",
        "\n",
        "> all types (aerated, rough, smooth)\n",
        "\n",
        "2. sand\n",
        "\n",
        ">  dry open and sparsely vegetated sand (aeolian, beach and channel margin)\n",
        "\n",
        "3. wet sand\n",
        "\n",
        "> wet \"intertidal\" channel margins adjacent to dry sand\n",
        "\n",
        "4. submerged sand\n",
        "\n",
        "> sand deposits visible through shallow water\n",
        "\n",
        "5. talus and bedrock\n",
        "\n",
        "> all bedrock and coarse material deposited under gravity\n",
        "\n",
        "6. debris fan\n",
        "\n",
        "> all coarse clast-supported sediment deposited by water\n",
        "\n",
        "7. muddy debris\n",
        "\n",
        "> all poorly sorted matrix-supported sediment deposited by water\n",
        "\n",
        "8. vegetation\n",
        "\n",
        "> medium and dense stands, low and canopy\n",
        "\n",
        "9. other\n",
        "\n",
        "> boats and beach umbrellas!\n",
        "\n",
        "\n",
        "\n",
        "### Code\n",
        "Several state-of-the-art models. Easily plug and play with different models\n",
        "\n",
        "The repository contains a python/tensorflow workflow for retraining numerous deep learning models for generic image semantic segmentation. It contains several state-of-the-art models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5ZCGkZTGgsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -rf Semantic-Segmentation-Suite/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rlMHsNAGhjj",
        "colab_type": "text"
      },
      "source": [
        "Head over to the \"files\" tab and hit \"refresh\". You'll see that the Semantic-Segmentation-Suite folder is gone.\n",
        "\n",
        "Next, use ```git clone``` to clone my github-hosted repository which contains the data and code for this workshop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfGMldJrEdZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone --depth 1 https://github.com/dbuscombe-usgs/Semantic-Segmentation-Suite.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa62qsTkII56",
        "colab_type": "text"
      },
      "source": [
        "## Navigating the file system and exploring the data\n",
        "\n",
        "We're going to change our working directory to that folder we just downloaded from github. We could do that using bash or python. Let's use python: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwqTCWWWEvti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('./Semantic-Segmentation-Suite')\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6uztn1TTxu5",
        "colab_type": "text"
      },
      "source": [
        "Next we're going to import the ```tensorflow``` module and print the version number. At the time of writing, the version number is ```1.13.1```. Be aware that it is possible this code could break with other (later/earlier) versions of tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwkOXgzgEwYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.VERSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJdWf8U2lqfd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The following bash command will tell you how much RAM you have available, in GB\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYq4ysgXlqlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! free -g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op0P-rSfKWDh",
        "colab_type": "text"
      },
      "source": [
        "You can test to see if a GPU is available like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sL4QAw1KWKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKW3PESJUUe4",
        "colab_type": "text"
      },
      "source": [
        "Let's explore the contents of our ```Semantic-Segmentation-Suite``` folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2k-emiVFSMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm0KXIZxFSP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls GeomorphA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OG0R-Vsuvtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls GeomorphB/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a50xG31ifeHj",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at the contents of ```class_dict.csv```, which contains the labels and associated RGB colors: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh9RBw5AFZ0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat GeomorphA/class_dict.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UczKeGW3uzEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat GeomorphB/class_dict.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toxVVo85f27d",
        "colab_type": "text"
      },
      "source": [
        "We can count the number of train/test/validation images using code such as:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WObLGp4kFZ2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "nval = len(glob('GeomorphA/val/*.jpg'))\n",
        "print('%i validation images' % nval)\n",
        "\n",
        "ntrain = len(glob('GeomorphA/train/*.jpg'))\n",
        "print('%i training images' % ntrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUHOzxtwu4Ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nval = len(glob('GeomorphB/val/*.jpg'))\n",
        "print('%i validation images' % nval)\n",
        "\n",
        "ntrain = len(glob('GeomorphB/train/*.jpg'))\n",
        "print('%i training images' % ntrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsIvtBFtSbPZ",
        "colab_type": "text"
      },
      "source": [
        "## Getting to know ```train.py``` and using it for quick/exploratory tests\n",
        "\n",
        "It is often useful to make quick tests to see if there is any major issue with the architecture of the model itself\n",
        "\n",
        "Let's execute the next bit of code, and while that is running we'll go through those command-line inputs\n",
        "\n",
        "We'll demonstrate this using the **GeomorphA** data set\n",
        "\n",
        "\n",
        "**12 mins**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOWG52_X6UOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -rf checkpoints/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA9wiW57SP9p",
        "colab_type": "text"
      },
      "source": [
        "Let's define some inputs and then run the script. While it's running, we'll talk about what it is doing\n",
        "\n",
        "**9 mins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLK5DiM7SQEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 10 ## number of epochs\n",
        "B = 1 ## batch size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-41bUFrQq9ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python3 train.py --num_epochs $N --dataset GeomorphA --num_val_images $nval --batch_size $B --frontend ResNet152"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhay0_AF10MU",
        "colab_type": "text"
      },
      "source": [
        "### train.py\n",
        "What is this script doing?\n",
        "\n",
        "First, it automatically downloads the model pre-trained checkpoints from the web\n",
        "\n",
        "> **Checkpoints?**\n",
        "\n",
        "Checkpoints are a way to save the current state of your model so that you can pick up from where you left off. They capture\n",
        "\n",
        "* The architecture of the model, allowing you to re-create the model\n",
        "\n",
        "* The weights of the model (for now, these will essentially be random numbers)\n",
        "\n",
        "* The training configuration (loss, optimizer, epochs, and other meta-information - I'll explain what all this means later)\n",
        "\n",
        "\n",
        "Then, it looks from images and corresponding label images, and trains a deep learning model **end-to-end**. This means that all the parameters in the models will be learned from our data, using a single large network that automatically learns how to map inputs (pixels and regions of images) to outputs (our classes). This is computationally intensive, hence the need for GPUs. Our workflow would therefore be described in the DL literature as 'end-to-end'. \n",
        "\n",
        "We will are not using 'transfer learning', which is the (usually less computationally intensive) process of using not only existing models, but also re-purposing existing parameters sets learned from other data.\n",
        "\n",
        "\n",
        "### Command-line inputs\n",
        "\n",
        "The ```train.py``` script takes a lot of command line inputs. For now, we're using most of the defaults and only specifying 4 things, which are described below. \n",
        "\n",
        "Later we will keep adding command line inputs that will specify which model(s) to use and how to treat the data, but for now, we are specifying: \n",
        "\n",
        "* The **batch size*** (1). This is the number of training images to work through before the model’s internal parameters are updated. In general, larger numbers are better. However, because our GPU memory is limited, so we are using only a batch size of 1.\n",
        "\n",
        "* The **number of epochs** (10). In the context of training a model, epoch is a term used to refer to one iteration where the model sees the whole training set to update its weights. \n",
        "\n",
        "So, with a batch size of 1 and 40+ training images, the model doesn't even see all the images. Given that image batches are drawn randomly, the minimum number of epochs for the model to see all images would be 40, but probably much larger. Typically, 100s to 1000s of epochs are used in model training\n",
        "\n",
        "* the dataset to use (\"GeomorphA\")\n",
        "\n",
        "* the number of validation images to use (all of them)\n",
        "\n",
        "* **Frontend** model or 'feature extractor'\n",
        "\n",
        "#### Feature Extractors\n",
        "\n",
        "Three feature extractors are implemented in the code we're using: \n",
        "\n",
        "* [MobileNetV2](https://arxiv.org/abs/1801.04381)\n",
        "* [ResNet50/101/152](https://arxiv.org/abs/1512.03385), and \n",
        "* [InceptionV4](https://arxiv.org/abs/1602.07261). \n",
        "\n",
        "A feature extractor in this case is a deep neural network architecture that has been optimized to automatically extract only those features in your data that contribute most to the prediction variable or output in which you are interested.\n",
        "\n",
        "We are using the feature extractor called  **ResNet152**, which is a slighter larger version of the default (ResNet101)\n",
        "\n",
        "[ResNet152](https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624) is a really popular generic model used in image analysis. The number refers to the number of layers, but the rest of the architecture is the same. More layers usually means more accuracy but slower training\n",
        "\n",
        "\n",
        "#### Dense (pixelwise) classifiers\n",
        "\n",
        "Once image features are extracted they are then further processed at different scales. Why? \n",
        "\n",
        "* processing the features at different scales will give the network the capacity to handle objects at different sizes\n",
        "\n",
        "* when performing segmentation there is a tradeoff. If you want good classification accuracy, then you’ll definitely want to process those high level features from later in the network since they are more discriminative and contain more useful semantic information. On the other hand, if you only process those deep features, you won’t get good localisation because of the low resolution!\n",
        "\n",
        "Fifteen segmentation models are implemented. See the [github repo](https://github.com/dbuscombe-usgs/Semantic-Segmentation-Suite) for full descriptions and links to the papers that describe them\n",
        "\n",
        "More details of these implementations can be found in [this](https://towardsdatascience.com/semantic-segmentation-with-deep-learning-a-guide-and-code-e52fc8958823) accessible blog post\n",
        "\n",
        "\n",
        "#### Initial checks\n",
        "\n",
        "You'll notice that there are 3 png files in the ```Semantic-Segmentation-Suite``` top-level directory, namely:\n",
        "\n",
        "* accuracy_vs_epochs.png\n",
        "* iou_vs_epochs.png\n",
        "* loss_vs_epochs.png\n",
        "\n",
        "These are made and updated every epoch by the program\n",
        "\n",
        "Double-click on them in the files tab and they will each pop out into their own window\n",
        "\n",
        "\n",
        "In the above, two important things to look for are\n",
        "\n",
        "* per-class accuracies. We want these to be fairly uniform across all classes. The accuracies will (for now) be low\n",
        "\n",
        "* average loss per epoch. Click on \"loss_vs_epochs.png\"\n",
        "\n",
        "In order to quantify how a given model performs, the loss function is usually used to evaluate to what extent the actual outputs are correctly predicted by the model outputs.\n",
        "\n",
        "\n",
        "### Looking at training results (Validation statistics)\n",
        "\n",
        "Double click on the images:\n",
        "\n",
        "* loss_vs_epochs.png\n",
        "* accuracy_vs_epochs.png\n",
        "* iou_vs_epochs.png\n",
        "\n",
        "\n",
        "True positives are image regions/pixels correctly classified as belonging to a certain class by the model\n",
        "\n",
        "True negatives are correctly classified as not belonging to a certain class.\n",
        "\n",
        "False negatives are regions/pixels incorrectly classified as not belonging to a certain class\n",
        "\n",
        "False positives are those regions/pixels incorrectly classified as belonging to a certain class.\n",
        "\n",
        "#### Accuracy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8E29hdoOfFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Math, HTML\n",
        "\n",
        "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
        "               \"latest.js?config=default'></script>\"))\n",
        "\n",
        "Math(r'A = \\frac{TP + TN}{TP + TN + FP + FN}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzDajObvOfg1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### F1 score (recall and precision)\n",
        "\n",
        "These metrics are commonly used in evaluation of pixelwise segmentations, where the number of pixels corresponding to each class vary considerably.\n",
        "\n",
        "Precision and recall are useful where the number of observations belonging to one class is significantly lower than those belonging to the other classes.\n",
        "\n",
        "##### Precision\n",
        "The proportion of positive identiﬁcations that are correct (a precision of 1 means there are no false positives)\n",
        "\n",
        "##### Recall\n",
        "Recall is the proportion of actual positives identiﬁed correctly (a recall of 1 means there are no false negatives)\n",
        "\n",
        "##### F1 score\n",
        "An equal weighting of the recall and precision, quantifying how well the model performs in general.\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1MxgyZqOjHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
        "               \"latest.js?config=default'></script>\"))\n",
        "\n",
        "Math(r'P = \\frac{TP + TN}{TP + FP}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CIbgC8CkpgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
        "               \"latest.js?config=default'></script>\"))\n",
        "Math(r'P = \\frac{TP + TN}{TP + FN}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBH-xtc8kwl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
        "               \"latest.js?config=default'></script>\"))\n",
        "Math(r'F_1 = 2\\frac{P \\times R}{P + R}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9D9TkQTOjRV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### IoU metric\n",
        "\n",
        "The intersection over union (IoU) metric is a simple metric used to evaluate the performance of a segmentation algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpk6ABiTCYHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
        "               \"latest.js?config=default'></script>\"))\n",
        "        \n",
        "Math(r'IoU = \\frac{y_{true} \\cap y_{pred}}{y_{true} \\cup y_{pred}}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJTKTpE7H0Sz",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.oreilly.com/library/view/deep-learning-for/9781788295628/assets/63fb2c41-8e83-49c5-ad3a-fee59e8a178b.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52ZrucNqDDY8",
        "colab_type": "text"
      },
      "source": [
        "By convention, a predicted bounding box is considered as being good if IoU >> 0.5\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/intersection-over-union.png)\n",
        "[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BzmPcLzfIsm",
        "colab_type": "text"
      },
      "source": [
        "### Looking at per-class accuracies, per image\n",
        "\n",
        "Below we're checking the contents of the ```checkpoints``` folder\n",
        "\n",
        "and then looking at the contents of a csv file that contains accuracies and other validation statistics for each category and each validation image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydu4MhV3fFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls checkpoints/\n",
        "! cat checkpoints/0009/val_scores.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVVSqY5V8HPW",
        "colab_type": "text"
      },
      "source": [
        "### Using checkpoints\n",
        "\n",
        "With Colab, GPU kernels may terminate, suddenly and without warning. Therefore, it is often a good idea to keep number of training epochs small, but to use checkpoints\n",
        "\n",
        "Add ```--continue_training True``` to the arguments list below if you want to retrain from a checkpoint\n",
        "\n",
        "**You can only use this option with one particular model** (but you can use a different feature extractor if you wish)\n",
        "\n",
        "Below we'll start from the last checkpoint, and train for an additional 10 epochs\n",
        "\n",
        "Like before, let's run the code first the talk about it while its running (scroll down)\n",
        "\n",
        "**9 mins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mYnPYWP8HWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python3 train.py --num_epochs $N --dataset GeomorphA --num_val_images $nval --batch_size $B --continue_training True --frontend ResNet152"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRJX9OIp-mGw",
        "colab_type": "text"
      },
      "source": [
        "The relatively minor disadvantage with the code we are using is that it doesn't keep track of the training history in the checkpoints, so our \"X_vs_epoch.png\" figures don't include the results from the previous model training step\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bx5JU20FGm5",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional neural networks\n",
        "\n",
        "While the model is training, let's explore in a little more detail convolutional neural networks, also known as **CNNs**, which are a specific type of neural networks used extensive in deep learning classification (and regression) tasks\n",
        "\n",
        "* Convolutional Neural Networks (CNNs) are very similar to ordinary Neural Networks: they are made up of neurons that have learnable weights and biases.\n",
        "\n",
        "* CNN architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture.\n",
        "\n",
        "Natural images exhibit ”‘stationarity”’, meaning that the statistics of one part of the image are the same as any other part. This suggests that the features that we learn at one part of the image can also be applied to other parts of the image, and we can use the same features at all locations.\n",
        "\n",
        "More precisely, having learned features over small (say 8x8) patches sampled randomly from the larger image, we can then apply this learned 8x8 feature detector anywhere in the image. Specifically, we can take the learned 8x8 features and ”‘convolve”’ them with the larger image, thus obtaining a different feature activation value at each location in the image.\n",
        "\n",
        "### Layers\n",
        "\n",
        "The layers consist of hierarchical filters that are designed to extract features of increasingly complexity\n",
        "\n",
        "* The input of each layer is the output of the previous one\n",
        "\n",
        "* The layer does not need to learn the whole concept at once, but actually build a chain of features that build that knowledge.\n",
        "\n",
        "* It learns the best way to map inputs to outputs (you don’t need to)\n",
        "\n",
        "\n",
        "![](https://i1.wp.com/www.michaelchimenti.com/wp-content/uploads/2017/11/Deep-Neural-Network-What-is-Deep-Learning-Edureka.png)\n",
        "\n",
        "\n",
        "### Types of layers\n",
        "\n",
        "CNNs are generally composed of the following layers:\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/architecture-cnn.png)\n",
        "[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
        "\n",
        "#### Convolution layer (CONV)\n",
        "\n",
        "[source](http://deeplearning.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/)\n",
        "\n",
        "The convolution layer (CONV) uses filters that perform convolution operations as it is scanning the input with respect to its dimensions. Its hyperparameters include the filter size F and stride S. The resulting output is called feature map or activation map.\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/convolution-layer-a.png)\n",
        "\n",
        "#### Pooling (POOL)\n",
        "\n",
        "The pooling layer (POOL) is a downsampling operation, typically applied after a convolution layer, which does some spatial invariance. In particular, max and average pooling are special kinds of pooling where the maximum and average value is taken, respectively.\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/max-pooling-a.png)\n",
        "[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
        "\n",
        "#### Fully Connected (FC)\n",
        "\n",
        "The fully connected layer (FC) operates on a flattened input where each input is connected to all neurons. If present, FC layers are usually found towards the end of CNN architectures and can be used to optimize objectives such as class scores.\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/fully-connected.png)\n",
        "[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARBeqelG1e18",
        "colab_type": "text"
      },
      "source": [
        "Let's take another look at the validation scores from the last epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh2v7JTcWkVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls checkpoints/\n",
        "! cat checkpoints/0009/val_scores.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVYucG2aQ9mx",
        "colab_type": "text"
      },
      "source": [
        "Looks reasonable despite only training for 20 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TOMW4wJifku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 30%; color: blue;'><b>Congratulations!</b></marquee>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcfKiH8PzwnP",
        "colab_type": "text"
      },
      "source": [
        "Before we go on, we should rename the checkpoints folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHOjnhrzzuiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv checkpoints/ checkpoints_geoA_resnet152_densenet56/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Q82dxw2cUG",
        "colab_type": "text"
      },
      "source": [
        "### Testing a different model (and data set)\n",
        "\n",
        "Like before, let's run the code first the talk about it while its running (scroll down)\n",
        "\n",
        "(You may get a \"running out of memory\" warning. Just hit **Ignore** - you should be ok)\n",
        "\n",
        "We'll demonstrate this using the **GeomorphB** data set\n",
        "\n",
        "**11 mins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwYb5_bw2dFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python3 train.py --num_epochs $N --dataset GeomorphB --num_val_images $nval --batch_size $B --frontend ResNet152 --model FC-DenseNet103 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi3Zx7yz-Yqx",
        "colab_type": "text"
      },
      "source": [
        "This time, we switched from the default model (\"FC-DenseNet56\") to a slightly larger version of the same model, called **FC-DenseNet103**\n",
        "\n",
        "Let's take a quick look at what that means\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*RfyAoe6Wlv4aLip2Y5Aw-Q.png)\n",
        "\n",
        "A standard approach is to pass the input image goes through multiple convolutions and obtain high-level features.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*4wx7szWCBse9-7eemGQJSw.png)\n",
        "\n",
        "In a **ResNet** architecture, each layer gets to see both the output from the previous layer (standard) as well as the inputs to that layer. So it not only sees the ouputs but also the data used to learn that output\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*rmHdoPjGUjRek6ozH7altw.png)\n",
        "\n",
        "A **Densenet** takes that concept even further: each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Each layer is receiving a “collective knowledge” from all preceding layers.\n",
        "\n",
        "Since each layer receives feature maps from all preceding layers, network can be thinner (few layers than it would ordinarily be). The result is higher computational and memory efficiencies. The following figure shows the concept of concatenation during forward propagation:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*9ysRPSExk0KvXR0AhNnlAA.gif)\n",
        "\n",
        "[source](https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHJEVdoB3cKm",
        "colab_type": "text"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Backpropagation\n",
        "Backpropagation is a method to update the weights in the neural network by taking into account the actual output and the desired output. \n",
        "\n",
        "### Updating weights\n",
        "In a neural network, weights are updated as follows:\n",
        "\n",
        "* Step 1: Take a batch of training data and perform forward propagation to compute the loss. \n",
        "* Step 2: Backpropagate the loss to get the gradient of the loss with respect to each weight. \n",
        "* Step 3: Use the gradients to update the weights of the network.\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/update-weights.png)\n",
        "[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks)\n",
        "\n",
        "### Optimizing convergence\n",
        "\n",
        "Stochastic Gradient Descent is a popular way to find how to minimize a cost function  (called finding a global minima)\n",
        "\n",
        "For each example in the data:\n",
        "\n",
        "* find the value predicted by the neural network \n",
        "* calculate the loss from the loss function \n",
        "* find partial derivatives of the loss function, these partial derivatives produce gradients\n",
        "* use the gradients to update the values of weights and biases\n",
        "\n",
        "A more detailed yet accessible explanation may be found [here](http://ruder.io/optimizing-gradient-descent/) \n",
        "\n",
        "#### Learning rate\n",
        "The learning rate, indicates at which pace the weights get updated. It can be fixed or adaptively changed. \n",
        "\n",
        "* If the learning rate is low, then training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny.\n",
        "\n",
        "* If the learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/0*QwE8M4MupSdqA3M4.png)\n",
        "\n",
        "[source](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b)\n",
        "\n",
        "\n",
        "#### Adaptive learning rates\n",
        "Letting the learning rate vary when training a model can reduce the training time and improve the numerical optimal solution. \n",
        "\n",
        "* The Adam optimizer is the most commonly used technique. \n",
        "\n",
        "* Here we are using a technique called Root Mean Squared Propagation\" or RMSprop an adaptive version of Stochastic Gradient Descent. A more detailed yet accessible explanation may be found [here](http://ruder.io/optimizing-gradient-descent/) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybd7Q85JIOvp",
        "colab_type": "text"
      },
      "source": [
        "In your own time, you'll be able to explore what \"front-end\" and \"model\" combinations are best\n",
        "\n",
        "\n",
        "Ok, so let's take a look at our validation scores once more so we can compare them to the previous set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrVABWUVKCq",
        "colab_type": "text"
      },
      "source": [
        "## A note on creating label images\n",
        "\n",
        "Myself and colleagues are working on a tool called [Earth Annotator](https://github.com/dbuscombe-usgs/EarthAnnotator) that uses supervised machine learning to quickly and efficiently create label imagery.\n",
        "\n",
        "* Details of the algorithms used are described in [this open access paper](https://www.mdpi.com/2076-3263/8/7/244/pdf).\n",
        "\n",
        "* In your own time, you can trial this technique by launching an ipython notebook from [here](https://mybinder.org/v2/gh/dbuscombe-usgs/EarthAnnotator/master?filepath=EarthAnnotator.ipynb). \n",
        "\n",
        "* The user is prompted to provide examples of all the categories in the image, using the cursor. The algorithm (called [a fully connected conditional random field](https://en.wikipedia.org/wiki/Conditional_random_field)) then models the likelihood of those labels for every unannotated pixel in the image, based on color and/or location.\n",
        "\n",
        "* This makes labeling a lot faster than doing it completely by hand\n",
        "\n",
        "* Optimizing this technique is an active research topic: please get in touch or submit a pull request on github if you have ideas to make it better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IQaulGm6IH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls checkpoints/\n",
        "! cat checkpoints/0009/val_scores.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFID_QfL7C4D",
        "colab_type": "text"
      },
      "source": [
        "## Managing and using checkpoints\n",
        "\n",
        "Before we go further, let's talk a little more about checkpoints\n",
        "\n",
        "\n",
        "### Viewing model predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy5IkxL57P-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls checkpoints/0009"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjOMqpiR7VDt",
        "colab_type": "text"
      },
      "source": [
        "In each of the subfolders of checkpoints, and for each validation image, there is a label image enabling us to visually compare the ground truth (suffix \"gt\") and the model prediction (suffix \"pred\") "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3Px_9777bMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp checkpoints/0009/B_tile_11264_0_gt.png exampleB_gt.png\n",
        "! cp checkpoints/0009/B_tile_11264_0_pred.png exampleB_pred.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wM4SLd-7gDH",
        "colab_type": "text"
      },
      "source": [
        "In your files list (remember to hit **Refresh**), you can double click on each image to view"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcN17n8477ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp checkpoints_geoA_resnet152_densenet56/0009/B_tile_11264_0_gt.png exampleA_gt.png\n",
        "! cp checkpoints_geoA_resnet152_densenet56/0009/B_tile_11264_0_pred.png exampleA_pred.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU_0cX4m77vd",
        "colab_type": "text"
      },
      "source": [
        "Afterwards, we can remove them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zabIDSw71fA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm example*.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5D9Jo1f8aCt",
        "colab_type": "text"
      },
      "source": [
        "Before we move on, let's take a look at another example validation image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdgr-Oqy8aKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp checkpoints/0009/A_tile_4608_10752_gt.png exampleB_gt.png\n",
        "! cp checkpoints/0009/A_tile_4608_10752_pred.png exampleB_pred.png\n",
        "! cp checkpoints_geoA_resnet152_densenet56/0009/A_tile_4608_10752_gt.png exampleA_gt.png\n",
        "! cp checkpoints_geoA_resnet152_densenet56/0009/A_tile_4608_10752_pred.png exampleA_pred.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7PyNqDW85Xy",
        "colab_type": "text"
      },
      "source": [
        "Ok, so it looks like we're making headway, but clearly we need to give the model more time (epochs)\n",
        "\n",
        "\n",
        "Unfortunately, we don't have that time in this class, so instead we'll download some checkpoints I made earlier and resume model training \n",
        "\n",
        "\n",
        "But before we do anything, let's do some housekeeping by removing those example files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4XrMXK185gO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm example*.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpPx8x3F9jJm",
        "colab_type": "text"
      },
      "source": [
        "### Saving and downloading checkpoints\n",
        "\n",
        "For consistency, we'll first rename our **GeomorphB** checkpoints using the same convention as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N9Bev8F-gML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv checkpoints/ checkpoints_geoB_resnet152_densenet103/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYi5urN9-gi3",
        "colab_type": "text"
      },
      "source": [
        "The next bit of code will remove all 10 checkpoint validation folders, leaving only the checkpoint files themselves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQZvdpeL9jV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!for i in `seq 0 9`; do rm -rf checkpoints_geoB_resnet152_densenet103/000$i; done\n",
        "!ls checkpoints_geoB_resnet152_densenet103"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEpfxGW5-Kkp",
        "colab_type": "text"
      },
      "source": [
        "And we'll do the same for the **GeomorphA** tests too"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWHMfEh0-uQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!for i in `seq 0 9`; do rm -rf checkpoints_geoA_resnet152_densenet56/000$i; done\n",
        "!ls checkpoints_geoA_resnet152_densenet56"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if4MLUsgGR4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Note that if you had more than 10 epochs, say 30, you'd also have to add a line like this:\n",
        "## !for i in `seq 10 29`; do rm -rf checkpoints/00$i; done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8oue9BP_quD",
        "colab_type": "text"
      },
      "source": [
        "The following command will zip up the contents of the checkpoint folders, for easy download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOLH5Yrt_q0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! zip -r checkpoints_geoA_resnet152_densenet56.zip checkpoints_geoA_resnet152_densenet56/*.*\n",
        "! zip -r checkpoints_geoB_resnet152_densenet103.zip checkpoints_geoB_resnet152_densenet103/*.*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUTA4jqiAZJ6",
        "colab_type": "text"
      },
      "source": [
        "If you like, you could check the file size in MB of the zipped folders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp6ofdHFAZRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! du -h checkpoints_geoB_resnet152_densenet103.zip\n",
        "! du -h checkpoints_geoA_resnet152_densenet56.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJs7ZcwT_Nm4",
        "colab_type": "text"
      },
      "source": [
        "Now we have zipped folders containing the checkpoints, we may download them locally for further use, on- or off-line.\n",
        "\n",
        "You can download it straight from the **files** browser on the left > right-click > selecting **download**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOGTm_FkHAei",
        "colab_type": "text"
      },
      "source": [
        "### Downloading and using a checkpoints file saved on google drive\n",
        "\n",
        "The following workflow would enable you to download that zipped checkpoint file from a google drive. First we'll define some functions for downloading (the details are not too important)\n",
        "\n",
        "[source for following code](https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9_RhBIMHCKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkLBu3zTHRxV",
        "colab_type": "text"
      },
      "source": [
        "The following code will download a zipped file from my google drive containing checkpoint files, unzip it into a folder, and then delete the zipped file   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL-z1okrHR3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# make the checkpoints folder if it doesn't already exist\n",
        "try:\n",
        "  os.mkdir(os.getcwd()+os.sep+'checkpoints')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "url = 'https://drive.google.com/file/d/1mS-ODCEGVct3QSEVkZsyNHQkO7SOa76k/view?usp=sharing'\n",
        "file_id = '1mS-ODCEGVct3QSEVkZsyNHQkO7SOa76k'  \n",
        "\n",
        "destination = 'csdms_checkpoints_geoA_resnet152_densenet103_200.zip'\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za8EKOB7Ieql",
        "colab_type": "text"
      },
      "source": [
        "Check on the size of the zipped file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LBWzPKjHhnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! stat --printf=\"%s\" csdms_checkpoints_geoA_resnet152_densenet103_200.zip\n",
        "! du -h csdms_checkpoints_geoA_resnet152_densenet103_200.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uc3Cm5QJLH3",
        "colab_type": "text"
      },
      "source": [
        " Then unzip it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQUimETbIuVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip csdms_checkpoints_geoA_resnet152_densenet103_200.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkGpG3r7JXAH",
        "colab_type": "text"
      },
      "source": [
        "If you're on a windows machine, you could uncomment and run the following python code instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnZZHKt_I_9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('unzipping')\t\n",
        "#zip_ref = zipfile.ZipFile(destination, 'r')\n",
        "#zip_ref.extractall(os.getcwd())\n",
        "#zip_ref.close()\n",
        "#os.remove(destination)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7XpkfprqiyY",
        "colab_type": "text"
      },
      "source": [
        "Double check the contents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6E0RY65qi4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls checkpoints/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1F2khnOJ1Eo",
        "colab_type": "text"
      },
      "source": [
        "## Model training -- this time for real (with my pre-prepared checkpoints)\n",
        "\n",
        "### GeomorphA\n",
        "\n",
        "This model checkpoint has undergone 200 iterations, but let's do another 5 and look at the results\n",
        "\n",
        "**6-7 mins**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BACwnIyoJ1Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python3 train.py --num_epochs 5 --dataset GeomorphA --continue_training True --num_val_images $nval --batch_size $B --frontend ResNet152 --model FC-DenseNet103"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-okiBvfuQjub",
        "colab_type": "text"
      },
      "source": [
        "You'll see after the first iteration that validation metrics have improved considerably, but that the model is still imperfect\n",
        "\n",
        "While that's running, let's discuss strategies that we might adopt to improve overall and specific class accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BudZDgnCk9er",
        "colab_type": "text"
      },
      "source": [
        "###  The dataset\n",
        "\n",
        "* There is error in the label data\n",
        "\n",
        "* Should there be other categories?\n",
        "\n",
        "* debris and sand both score relatively poorly -- are these categories too large?\n",
        "\n",
        "\n",
        "The **lumping or splitting?** conundrum: often the hardest thing to get right with an image classification task\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amSdkSgut6ay",
        "colab_type": "text"
      },
      "source": [
        "Some insight into this may be obtained when we examine the results of **GeomorphB**, which has split sand and debris categories\n",
        "\n",
        "First, let's rename the checkpoints folder, then take a look at some examples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_vLsD60t6gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv checkpoints/ checkpoints_geoA_resnet152_densenet103_205epochs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjrlyHyBur-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls checkpoints_geoA_resnet152_densenet103_205epochs/0004\n",
        "! cat checkpoints_geoA_resnet152_densenet103_205epochs/0004/val_scores.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72NI4IDD5bud",
        "colab_type": "text"
      },
      "source": [
        "Overall some categories are predicted really well:\n",
        "* Water ~95%\n",
        "* Vegetation > 90%\n",
        "* Boats/umbrellas > 90%\n",
        "\n",
        "Others, not so good:\n",
        "* Bedrock + talus > 70%\n",
        "* Sand ~ 50%\n",
        "* Debris ~ 40%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoNWnkW7uiMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp checkpoints_geoA_resnet152_densenet103_205epochs/0004/D_tile_3584_2048_gt.png geoA_ex1_gt.png\n",
        "! cp checkpoints_geoA_resnet152_densenet103_205epochs/0004/D_tile_3584_2048_pred.png geoA_ex1_pred.png\n",
        "! cp GeomorphA/val/D_tile_3584_2048.jpg geo_ex1.jpg\n",
        "\n",
        "! cp checkpoints_geoA_resnet152_densenet103_205epochs/0004/A_tile_2560_15872_gt.png geoA_ex2_gt.png\n",
        "! cp checkpoints_geoA_resnet152_densenet103_205epochs/0004/A_tile_2560_15872_pred.png geoA_ex2_pred.png\n",
        "! cp GeomorphA/val/A_tile_2560_15872.jpg geo_ex2.jpg\n",
        "\n",
        "! cp checkpoints_geoA_resnet152_densenet103_205epochs/0004/A_tile_5632_7680_gt.png geoA_ex3_gt.png\n",
        "! cp checkpoints_geoA_resnet152_densenet103_205epochs/0004/A_tile_5632_7680_pred.png geoA_ex3_pred.png\n",
        "! cp GeomorphA/val/A_tile_5632_7680.jpg geo_ex3.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vVpMFWbwqwm",
        "colab_type": "text"
      },
      "source": [
        "### GeomorphB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_etpPQS5w3L6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make the checkpoints folder if it doesn't already exist\n",
        "try:\n",
        "  os.mkdir(os.getcwd()+os.sep+'checkpoints')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "url = 'https://drive.google.com/file/d/1oq7WYItW38uZL4es1FRXdGq7vE_0tXu1/view?usp=sharing'\n",
        "file_id = '1oq7WYItW38uZL4es1FRXdGq7vE_0tXu1'  \n",
        "\n",
        "destination = 'csdms_checkpoints_geoB_resnet152_densenet103_200.zip'\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPU4ya2fw33U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip csdms_checkpoints_geoB_resnet152_densenet103_200.zip\n",
        "! ls checkpoints/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu8XguuXw8yP",
        "colab_type": "text"
      },
      "source": [
        "This model checkpoint has undergone 200 iterations, but let's do another 5 and look at the results\n",
        "\n",
        "**9 mins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1GQ3Itaw_vg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python3 train.py --num_epochs 5 --dataset GeomorphB --continue_training True --num_val_images $nval --batch_size $B --frontend ResNet152 --model FC-DenseNet103"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUwW9KrXsk0q",
        "colab_type": "text"
      },
      "source": [
        "### About the model training\n",
        "\n",
        "I didn't test all the models and feature extractors. There could be better models. \n",
        "\n",
        "Similarly, image augmentation could be explored\n",
        "\n",
        "## Using image augmentation\n",
        "\n",
        "Deep learning models usually need a lot of data to be properly trained. It is often useful to get more data from the existing ones using data augmentation techniques. The main ones are summed up in the table below. More precisely, given the following input image, here are the techniques that we can apply:\n",
        "\n",
        "* Flip (horizontal or vertical). In the above, I have implemented both\n",
        "\n",
        "* Rotation. \n",
        "\n",
        "* Brightness.  \n",
        "\n",
        "Other codes might offer other augmentation tricks such as:\n",
        "\n",
        "* Information loss\n",
        "* Random cropping (subsets of images)\n",
        "* Contrast\n",
        "* Random zoom\n",
        "* etc\n",
        "\n",
        "In your own time, uncomment and run the following code to see if validation results improve: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axh5hYegq5Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##! python3 train.py --num_epochs 10 --dataset Geomorph --num_val_images 14 --batch_size 1 --continue_training True --frontend ResNet152 --model FC-DenseNet103 --h_flip True --v_flip True --brightness 0.1 --rotation 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aBE1LGKxhoI",
        "colab_type": "text"
      },
      "source": [
        "Let's rename the folder and check the contents of the last validation set, just like we did before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Uph-CIxhzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv checkpoints/ checkpoints_geoB_resnet152_densenet103_205epochs/\n",
        "! ls checkpoints_geoB_resnet152_densenet103_205epochs/0004\n",
        "! cat checkpoints_geoB_resnet152_densenet103_205epochs/0004/val_scores.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0KTGhXvx225",
        "colab_type": "text"
      },
      "source": [
        "Then, for comparison, we'll take a look at the same 3 validation images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eue0fArYx29Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp checkpoints_geoB_resnet152_densenet103_205epochs/0004/D_tile_3584_2048_gt.png geoB_ex1_gt.png\n",
        "! cp checkpoints_geoB_resnet152_densenet103_205epochs/0004/D_tile_3584_2048_pred.png geoB_ex1_pred.png\n",
        "\n",
        "! cp checkpoints_geoB_resnet152_densenet103_205epochs/0004/A_tile_2560_15872_gt.png geoB_ex2_gt.png\n",
        "! cp checkpoints_geoB_resnet152_densenet103_205epochs/0004/A_tile_2560_15872_pred.png geoB_ex2_pred.png\n",
        "\n",
        "! cp checkpoints_geoB_resnet152_densenet103_205epochs/0004/A_tile_5632_7680_gt.png geoB_ex3_gt.png\n",
        "! cp checkpoints_geoB_resnet152_densenet103_205epochs/0004/A_tile_5632_7680_pred.png geoB_ex3_pred.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muK0D6Ifquv2",
        "colab_type": "text"
      },
      "source": [
        "Some **GeomorphB** average accuracies were very similar **GeomorphA**, for example:\n",
        "\n",
        "* water ~ 0.95\n",
        "* bedrocktalus ~ 0.7\n",
        "* veg ~ 0.9\n",
        "* other ~ 0.9\n",
        "\n",
        "Values for sand were generally not good:\n",
        "\n",
        "* sand ~ 0.4\n",
        "* wetsand ~ 0.4\n",
        "* submerged_sand ~ 0.5\n",
        "\n",
        "* Average debrisfan accuracy was marginally better ~ 0.45 \n",
        "\n",
        "* But muddy debris accuracy was poor at ~ 0.3\n",
        "\n",
        "\n",
        "DL workflows are scalable and should keep getting better with more and more data. \n",
        "\n",
        "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/08/Why-Deep-Learning-1024x742.png)\n",
        "\n",
        "[source: Andrew Ng ](https://www.slideshare.net/ExtractConf)\n",
        "\n",
        "But how much is enough? Clearly in this case we need a bigger data set\n",
        "\n",
        "\n",
        "## GeomorphB loss, accuracy and IoU curves\n",
        "\n",
        "![](https://github.com/dbuscombe-usgs/cdi_dl_workshop/raw/67e8c84d0e0b89b024814ef2e8f3bed091ee0c4e/Day2/figs/Picture1.png)\n",
        "\n",
        "#### Loss vs Epochs            \n",
        "\n",
        "![alt text-1](https://user-images.githubusercontent.com/3596509/57955589-351aa900-78ee-11e9-8944-ff7769a1b645.png)\n",
        "\n",
        "\n",
        "#### Accuracy vs Epochs            \n",
        "\n",
        "![alt text-2](https://user-images.githubusercontent.com/3596509/57955613-419f0180-78ee-11e9-80f4-5ada1fc346e8.png)\n",
        "\n",
        "#### IoU vs Epochs            \n",
        "\n",
        "![alt text-1](https://user-images.githubusercontent.com/3596509/57955618-4368c500-78ee-11e9-8131-45cb5a594b6d.png)  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfVAOvsUJxvi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### General limitations\n",
        "\n",
        "* It can be very difficult to interpret a model produced with deep learning. Such models may have many layers and thousands of nodes; interpreting each of these individually is impossible\n",
        "\n",
        "* We therefore evaluate deep learning models by measuring how well they predict, treating the architecture itself as a “black box\"\n",
        "\n",
        "* DL models require a great deal of computing power to build. For simpler problems with small data sets, deep learning may not produce sufficient added benefit over simpler methods to justify the cost and time.\n",
        "\n",
        "* DL models are very data hungry\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzxQbjptlOkh",
        "colab_type": "text"
      },
      "source": [
        "### Deep learning in the geosciences\n",
        "\n",
        "\n",
        "* It is very early days in the development and understanding of the role of DL models in the geosciences\n",
        "\n",
        "* Many claims about the efficacy of DCNNs for image classification are largely based upon analyses of conventional photographic imagery of familiar, mostly anthropogenic objects. Much more work required for the image classification of natural textures and objects.\n",
        "\n",
        "* We need to build our own **shared databases**\n",
        "\n",
        "* We need robust **benchmarks**\n",
        "\n",
        "* We need guidance on **best practices**\n",
        "\n",
        "Finally ...\n",
        "\n",
        "> \"There is an ongoing misconception that AI/ML are intrinsically valuable, and that therefore working in the field is bound to make you rich. A ML model is only as valuable as the problem it solves. ML without an application isn't worth anything (beyond intellectual curiosity).\" Francois Chollet (author of Keras). [source](https://twitter.com/fchollet/status/1130000985466626048)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXoOc5Rklpkw",
        "colab_type": "text"
      },
      "source": [
        "# Clinic: Landcover and landform classification using deep neural networks\n",
        "\n",
        "\n",
        "CSDMS\t2019\tAnnual\tMeeting:\tCSDMS\t3.0\t– Bridging\tBoundaries\n",
        "\n",
        "May\t21-23,\t2019\n",
        "\n",
        "### Daniel Buscombe\n",
        "\n",
        "Assistant Research Professor\n",
        "\n",
        "School of Earth and Sustainability\n",
        "\n",
        "School of Informatics, Computing and Cyber Systems\n",
        "\n",
        "\n",
        "Northern Arizona University, Flagstaff, AZ\n",
        "\n",
        "[Email](mailto:daniel.buscombe@nau.edu)\n",
        "[Web](www.danielbuscombe.com)\n",
        "[Google Scholar](https://scholar.google.com/citations?user=bwVl0NwAAAAJ&hl=en)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu6lHLuUlqGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}